{
  "attributes": [
    {
      "default": "nil",
      "size": null,
      "type": "bigint",
      "source": "id",
      "references": null,
      "allow_nil?": false,
      "primary_key?": true,
      "generated?": true
    },
    {
      "default": "nil",
      "size": null,
      "type": "text",
      "source": "state",
      "references": null,
      "allow_nil?": false,
      "primary_key?": false,
      "generated?": false
    },
    {
      "default": "nil",
      "size": null,
      "type": "text",
      "source": "openai_input_file_id",
      "references": null,
      "allow_nil?": true,
      "primary_key?": false,
      "generated?": false
    },
    {
      "default": "nil",
      "size": null,
      "type": "text",
      "source": "openai_output_file_id",
      "references": null,
      "allow_nil?": true,
      "primary_key?": false,
      "generated?": false
    },
    {
      "default": "nil",
      "size": null,
      "type": "text",
      "source": "openai_error_file_id",
      "references": null,
      "allow_nil?": true,
      "primary_key?": false,
      "generated?": false
    },
    {
      "default": "nil",
      "size": null,
      "type": "text",
      "source": "openai_batch_id",
      "references": null,
      "allow_nil?": true,
      "primary_key?": false,
      "generated?": false
    },
    {
      "default": "nil",
      "size": null,
      "type": "utc_datetime",
      "source": "openai_status_last_checked_at",
      "references": null,
      "allow_nil?": true,
      "primary_key?": false,
      "generated?": false
    },
    {
      "default": "nil",
      "size": null,
      "type": "bigint",
      "source": "openai_requests_completed",
      "references": null,
      "allow_nil?": true,
      "primary_key?": false,
      "generated?": false
    },
    {
      "default": "nil",
      "size": null,
      "type": "bigint",
      "source": "openai_requests_failed",
      "references": null,
      "allow_nil?": true,
      "primary_key?": false,
      "generated?": false
    },
    {
      "default": "nil",
      "size": null,
      "type": "bigint",
      "source": "openai_requests_total",
      "references": null,
      "allow_nil?": true,
      "primary_key?": false,
      "generated?": false
    },
    {
      "default": "nil",
      "size": null,
      "type": "text",
      "source": "url",
      "references": null,
      "allow_nil?": false,
      "primary_key?": false,
      "generated?": false
    },
    {
      "default": "nil",
      "size": null,
      "type": "text",
      "source": "model",
      "references": null,
      "allow_nil?": false,
      "primary_key?": false,
      "generated?": false
    },
    {
      "default": "0",
      "size": null,
      "type": "bigint",
      "source": "request_count",
      "references": null,
      "allow_nil?": false,
      "primary_key?": false,
      "generated?": false
    },
    {
      "default": "0",
      "size": null,
      "type": "bigint",
      "source": "size_bytes",
      "references": null,
      "allow_nil?": false,
      "primary_key?": false,
      "generated?": false
    },
    {
      "default": "0",
      "size": null,
      "type": "bigint",
      "source": "estimated_input_tokens_total",
      "references": null,
      "allow_nil?": false,
      "primary_key?": false,
      "generated?": false
    },
    {
      "default": "0",
      "size": null,
      "type": "bigint",
      "source": "estimated_request_input_tokens_total",
      "references": null,
      "allow_nil?": false,
      "primary_key?": false,
      "generated?": false
    },
    {
      "default": "nil",
      "size": null,
      "type": "utc_datetime",
      "source": "capacity_last_checked_at",
      "references": null,
      "allow_nil?": true,
      "primary_key?": false,
      "generated?": false
    },
    {
      "default": "nil",
      "size": null,
      "type": "text",
      "source": "capacity_wait_reason",
      "references": null,
      "allow_nil?": true,
      "primary_key?": false,
      "generated?": false
    },
    {
      "default": "nil",
      "size": null,
      "type": "utc_datetime",
      "source": "waiting_for_capacity_since_at",
      "references": null,
      "allow_nil?": true,
      "primary_key?": false,
      "generated?": false
    },
    {
      "default": "nil",
      "size": null,
      "type": "text",
      "source": "error_msg",
      "references": null,
      "allow_nil?": true,
      "primary_key?": false,
      "generated?": false
    },
    {
      "default": "0",
      "size": null,
      "type": "bigint",
      "source": "token_limit_retry_attempts",
      "references": null,
      "allow_nil?": true,
      "primary_key?": false,
      "generated?": false
    },
    {
      "default": "nil",
      "size": null,
      "type": "utc_datetime",
      "source": "token_limit_retry_next_at",
      "references": null,
      "allow_nil?": true,
      "primary_key?": false,
      "generated?": false
    },
    {
      "default": "nil",
      "size": null,
      "type": "text",
      "source": "token_limit_retry_last_error",
      "references": null,
      "allow_nil?": true,
      "primary_key?": false,
      "generated?": false
    },
    {
      "default": "nil",
      "size": null,
      "type": "bigint",
      "source": "input_tokens",
      "references": null,
      "allow_nil?": true,
      "primary_key?": false,
      "generated?": false
    },
    {
      "default": "nil",
      "size": null,
      "type": "bigint",
      "source": "cached_tokens",
      "references": null,
      "allow_nil?": true,
      "primary_key?": false,
      "generated?": false
    },
    {
      "default": "nil",
      "size": null,
      "type": "bigint",
      "source": "reasoning_tokens",
      "references": null,
      "allow_nil?": true,
      "primary_key?": false,
      "generated?": false
    },
    {
      "default": "nil",
      "size": null,
      "type": "bigint",
      "source": "output_tokens",
      "references": null,
      "allow_nil?": true,
      "primary_key?": false,
      "generated?": false
    },
    {
      "default": "nil",
      "size": null,
      "type": "utc_datetime",
      "source": "expires_at",
      "references": null,
      "allow_nil?": true,
      "primary_key?": false,
      "generated?": false
    },
    {
      "default": "nil",
      "size": null,
      "type": "utc_datetime_usec",
      "source": "created_at",
      "references": null,
      "allow_nil?": false,
      "primary_key?": false,
      "generated?": false
    },
    {
      "default": "nil",
      "size": null,
      "type": "utc_datetime_usec",
      "source": "updated_at",
      "references": null,
      "allow_nil?": false,
      "primary_key?": false,
      "generated?": false
    }
  ],
  "table": "batches",
  "hash": "8D3A698E7C365A0C07B098ACEBEFD41B617713A4AA198B586C1E1D5CE5BB3240",
  "repo": "Elixir.Batcher.Repo",
  "multitenancy": {
    "global": null,
    "attribute": null,
    "strategy": null
  },
  "identities": [],
  "strict?": false,
  "custom_indexes": [
    {
      "message": null,
      "name": "batches_state_model_url_index",
      "table": null,
      "include": null,
      "where": null,
      "fields": [
        "state",
        "model",
        "url"
      ],
      "unique": false,
      "using": null
    }
  ],
  "custom_statements": [
    {
      "name": "backfill_batch_counters",
      "up": "UPDATE batches\nSET\n  request_count = (\n    SELECT COUNT(*)\n    FROM requests\n    WHERE requests.batch_id = batches.id\n  ),\n  estimated_input_tokens_total = COALESCE((\n    SELECT SUM(estimated_input_tokens)\n    FROM requests\n    WHERE requests.batch_id = batches.id\n  ), 0),\n  estimated_request_input_tokens_total = COALESCE((\n    SELECT SUM(estimated_request_input_tokens)\n    FROM requests\n    WHERE requests.batch_id = batches.id\n  ), 0),\n  size_bytes = COALESCE((\n    SELECT SUM(request_payload_size)\n    FROM requests\n    WHERE requests.batch_id = batches.id\n  ), 0)\n",
      "down": "SELECT 1;",
      "code?": false
    }
  ],
  "base_filter": null,
  "has_create_action": true
}